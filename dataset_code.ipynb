{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8fa191-dd7b-4c10-931d-a03b567fc310",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset Description: Musical EdTech Simulation\n",
    "\n",
    "## Overview\n",
    "This dataset is a simulated representation of a **Musical EdTech platform**, aiming to analyze user behavior, retention patterns, and premium conversion rates. It emulates a **real-life scenario** with added complexity, including realistic user attributes, engagement trends, and outliers. \n",
    "\n",
    "The dataset's core question is:\n",
    "**To what extent specific factorsâ€”such as age, instruments, or geographic location - affect retention and premium conversion rates on the platform?**\n",
    "\n",
    "## Objectives\n",
    "The dataset is designed to:\n",
    "1. **Analyze User Retention**:\n",
    "   - Investigate how retention varies by age, instrument, and country.\n",
    "   - Identify the impact of onboarding features on short-term (Day 1) and long-term (Day 30) retention.\n",
    "\n",
    "2. **Evaluate Premium Conversion**:\n",
    "   - Explore which user groups (e.g., age, geography) are more likely to convert to premium subscriptions.\n",
    "   - Study the relationship between engagement metrics (e.g., lessons completed) and conversions.\n",
    "\n",
    "3. **Test Robustness Against Outliers**:\n",
    "   - Include outliers to simulate unexpected but realistic scenarios, such as power users, extreme engagement levels, and geographic anomalies.\n",
    "\n",
    "## Dataset Features\n",
    "1. **Attributes**:\n",
    "   - **Age Group**: Simulates user demographics, with adjustments for seasonal trends like back-to-school effects.\n",
    "   - **Skill Level**: Captures user proficiency across Beginner, Intermediate, and Advanced levels.\n",
    "   - **Instrument**: Reflects the diversity of user interests in learning Guitar, Piano, Ukulele, Bass, or Voice.\n",
    "   - **Country**: Represents a global audience with varying economic conditions.\n",
    "\n",
    "2. **Retention Metrics**:\n",
    "   - **Day 1, Day 7, Day 30 Retention**: Tracks user engagement over time.\n",
    "\n",
    "3. **Churn**:\n",
    "   - Models disengagement based on age, instrument, and retention behavior.\n",
    "\n",
    "4. **Premium Conversion**:\n",
    "   - Simulates the likelihood of users upgrading to premium, influenced by multiple factors like age, country, retention, and engagement.\n",
    "\n",
    "5. **Outliers**:\n",
    "   - **Power Users**: Users with unusually high engagement (e.g., completing 50+ lessons).\n",
    "   - **Geographic Anomalies**: Rare cases with unexpectedly high or low premium conversions.\n",
    "   - **Instrument Extremes**: Users exhibiting disproportionate activity in specific instruments.\n",
    "\n",
    "## Real-Life Simulation\n",
    "The dataset reflects **real-world complexity** by:\n",
    "- Adjusting activity trends based on **day-of-week and seasonal patterns**.\n",
    "- Modeling demographic-specific behaviors, such as **higher retention in older users** and **greater conversions in stronger economies**.\n",
    "- Including **random variability and outliers** to test the robustness of analytical models.\n",
    "- Accounting for **cross-feature relationships** like age influencing instrument choice or country impacting retention.\n",
    "\n",
    "This dataset was conceived with the help of AI to ensure realistic variability while integrating the creator's background in **teaching, music, and data analysis**. It serves as an ideal testbed for advanced analytics, hypothesis testing, and predictive modeling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9fc408-f773-4ba9-94b2-8adc81919371",
   "metadata": {},
   "source": [
    "# Constants and Initial Setup\n",
    "This section initializes the dataset constants, such as the total number of users, date range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ad3b03b-f683-444d-8ecb-6a66d23594ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries succesfully imported\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "print(f\"Libraries succesfully imported\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39328afe-fca0-467b-94c4-d23e4b2a15dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution properly scaled for [0.12, 0.57, 0.43, 0.06]\n",
      "Corrected distribution: [0.1016949152542373, 0.4830508474576271, 0.364406779661017, 0.05084745762711865]\n",
      "\n",
      "\n",
      "Distribution properly scaled for [0.69, 0.22, 0.14]\n",
      "Corrected distribution: [0.657142857142857, 0.20952380952380953, 0.13333333333333333]\n",
      "\n",
      "\n",
      "Distribution properly scaled for [0.58, 0.27, 0.18, 0.11, 0.11]\n",
      "Corrected distribution: [0.46399999999999997, 0.21600000000000003, 0.144, 0.088, 0.088]\n",
      "\n",
      "\n",
      "Distribution properly scaled for [13.65, 8.0, 9.36, 5.25, 6.0, 5.0, 5.949999999999999, 3.7199999999999998, 6.15, 6.25, 5.6499999999999995, 3.3000000000000003, 3.54, 3.5999999999999996, 2.34, 2.38, 5.300000000000001, 1.21]\n",
      "Corrected distribution: [0.14123124676668392, 0.08277289187790998, 0.09684428349715467, 0.05431971029487843, 0.062079668908432487, 0.05173305742369374, 0.06156233833419554, 0.03848939472322814, 0.0636316606311433, 0.06466632177961718, 0.05845835488877392, 0.03414381789963787, 0.03662700465597517, 0.037247801345059485, 0.024211070874288667, 0.024624935333678218, 0.05483704086911537, 0.012519399896533884]\n",
      "\n",
      "\n",
      "Country weights: {'United States': 14.123124676668391, 'United Kingdom': 8.277289187790998, 'Germany': 9.684428349715466, 'Finland': 5.431971029487843, 'France': 6.207966890843249, 'Sweden': 5.173305742369374, 'Norway': 6.156233833419554, 'Brazil': 3.848939472322814, 'Australia': 6.363166063114329, 'Netherlands': 6.466632177961717, 'Canada': 5.845835488877392, 'Italy': 3.414381789963787, 'Spain': 3.6627004655975166, 'Ireland': 3.7247801345059486, 'Argentina': 2.4211070874288665, 'South Africa': 2.4624935333678217, 'New Zealand': 5.483704086911537, 'Mexico': 1.2519399896533885}\n",
      "\n",
      "Constants succesfully created\n"
     ]
    }
   ],
   "source": [
    "# Constants for dataset\n",
    "SEED = 301 \n",
    "np.random.seed(SEED) \n",
    "n_users = 30848  # Total number of users\n",
    "round_float = lambda x, y: round(np.random.uniform(x,y),2)\n",
    "def feat_scale(no_scale):\n",
    "    total = sum(no_scale)\n",
    "    if total == 1:\n",
    "        print(f\"Distribution already correct for {no_scale}\")\n",
    "        print('\\n') \n",
    "        return no_scale  # No need to scale\n",
    "    else:\n",
    "        print(f\"Distribution properly scaled for {no_scale}\")\n",
    "        # Scale to ensure the sum is 1\n",
    "        scale = [x / total for x in no_scale]\n",
    "        print(f\"Corrected distribution: {scale}\")\n",
    "        print('\\n')\n",
    "        return scale\n",
    "     \n",
    "start_date_range = pd.date_range(start=\"2024-08-01\", end=\"2024-08-31\")\n",
    "age_distribution_no_scale = [round_float(0.05, 0.25), round_float(0.45, 0.65), round_float(0.30, 0.50), round_float(0.02, 0.10)]\n",
    "age_distribution = feat_scale(age_distribution_no_scale)\n",
    "\n",
    "skill_distribution_no_scale = [round_float(0.55, 0.75), round_float(0.15, 0.30), round_float(0.05, 0.15)]\n",
    "skill_distribution = feat_scale(skill_distribution_no_scale)\n",
    "instrument_distribution_no_scale = [\n",
    "    round_float(0.3 + random.uniform(0, 0.1), 0.6),\n",
    "    round_float(0.15 + random.uniform(0, 0.05), 0.3),\n",
    "    round_float(0.1, 0.2 + random.uniform(0, 0.1)),\n",
    "    round_float(0.05, 0.2),\n",
    "    round_float(0.02, 0.15)\n",
    "]\n",
    "instrument_distribution = feat_scale(instrument_distribution_no_scale)\n",
    " \n",
    "country_weight_dict = {\"United States\":13, \"United Kingdom\":8, \"Germany\":8, \"Finland\": 5, \"France\": 5, \"Sweden\": 5, \"Norway\": 5,\n",
    "     \"Brazil\": 3, \"Australia\": 5, \"Netherlands\": 5, \"Canada\": 5,  \"Italy\": 3, \"Spain\": 3, \"Ireland\": 3, \"Argentina\": 2, \n",
    "    \"South Africa\": 2, \"New Zealand\": 5, \"Mexico\": 1}\n",
    "\n",
    "country_list = [country for country in country_weight_dict.keys()] \n",
    "\n",
    "country_weights = [(weight * round_float(1, 1.25)) for weight in country_weight_dict.values()] \n",
    "country_weights = [weight*100 for weight in feat_scale(country_weights)]\n",
    "\n",
    "country_weight_scale = {country:scale for country,scale in zip(country_list,country_weights)}  \n",
    "\n",
    "print(f\"Country weights: {country_weight_scale}\\n\")\n",
    "print(f\"Constants succesfully created\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9ae5ae-406c-403c-97cd-8641a415c901",
   "metadata": {},
   "source": [
    "# Simulating Real-Life Date Distributions\n",
    "User activity typically varies by the day of the week, with weekdays seeing higher engagement. Additionally, mid/late August may see a drop due to seasonal patterns. This section normalizes user distribution while maintaining realistic patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a57016d9-962a-481f-badf-a31b3cfb6277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily weights: [0.962580630737286, 0.8197100384922145, 0.30138974136848257, 0.4927688110516895, 0.5565537929689999, 0.6033393066065744, 0.7290139219254756, 0.5415240091039004, 0.8980186724925285, 0.6066200120787864, 0.7534722798657969, 1.081052634696308, 0.652291281783919, 0.6286641896095017, 0.9487465110505584, 0.812274605658382, 0.33120238763919335, 0.3732939688133825, 0.7712310805299213, 1.1442420369211816, 0.6510221353621124, 1.1854552312627833, 0.777030570208064, 0.4995371852564891, 0.7299428232222319, 1.036954957808348, 0.697731730009278, 1.0254070611309376, 0.5249336610041005, 0.9388767556148099, 0.70417575710165]\n",
      "\n",
      "Late August decay: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]\n",
      "\n",
      "Daily weights after: [0.96258063 0.81971004 0.30138974 0.49276881 0.55655379 0.60333931\n",
      " 0.72901392 0.54152401 0.89801867 0.60662001 0.75347228 1.08105263\n",
      " 0.65229128 0.62866419 0.94874651 0.73104715 0.29808215 0.33596457\n",
      " 0.69410797 1.02981783 0.58591992 1.06690971 0.69932751 0.44958347\n",
      " 0.65694854 0.93325946 0.62795856 0.92286636 0.47244029 0.84498908\n",
      " 0.63375818]\n",
      "\n",
      "Dates generated succesfully\n"
     ]
    }
   ],
   "source": [
    "SEED = 301 \n",
    "np.random.seed(SEED)\n",
    "# Simulating real-life date distributions (weekdays more active)\n",
    "daily_weights = [\n",
    "    random.uniform(0.5, 1.2) if date.weekday() < 5 else random.uniform(0.3, 0.8)\n",
    "    for date in start_date_range\n",
    "]\n",
    "print(f\"Daily weights: {daily_weights}\\n\") \n",
    "# Introduce mid/late August drop\n",
    "late_august_decay = [0.9 if date > pd.Timestamp(\"2024-08-15\") else 1 for date in start_date_range]\n",
    "print(f\"Late August decay: {late_august_decay}\\n\")\n",
    "daily_weights = np.array(daily_weights) * late_august_decay\n",
    "print(f\"Daily weights after: {daily_weights}\\n\")\n",
    "# Normalize daily weights to sum to total number of users\n",
    "daily_weights = daily_weights / daily_weights.sum() * n_users\n",
    "daily_counts = np.round(daily_weights).astype(int)\n",
    "\n",
    "\n",
    "# Ensure the counts sum to the total number of users\n",
    "daily_counts[-1] += n_users - daily_counts.sum()\n",
    "\n",
    "# Generate start dates based on daily counts\n",
    "start_dates = np.hstack([[date] * count for date, count in zip(start_date_range, daily_counts)])\n",
    "if len(start_dates) == n_users:\n",
    "    print(f\"Dates generated succesfully\")\n",
    "else:\n",
    "    print(f\"Inconsistent number of dates\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4643fb66-a8e7-449e-a2a0-2fdf759737a7",
   "metadata": {},
   "source": [
    "# Generating User Attributes\n",
    "This section assigns attributes to users, such as:\n",
    "- Age group\n",
    "- Skill level\n",
    "- Instrument. These distributions follow predefined probabilities but are adjusted for real-life scenarios like the back-to-school drop for `\"Under 18\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccfbd7f0-b219-40bd-b31b-58b2537a5bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes succesfully generated\n"
     ]
    }
   ],
   "source": [
    "SEED = 301 \n",
    "np.random.seed(SEED)\n",
    "# Assign age groups with back-to-school adjustment\n",
    "back_to_school_drop = 0.1  # 10% drop during mid/late August\n",
    "age_distribution_adjusted = age_distribution.copy()\n",
    "# Prevent any negative probabilities\n",
    "if age_distribution_adjusted[0] > back_to_school_drop:\n",
    "    age_distribution_adjusted[0] -= back_to_school_drop\n",
    "else:\n",
    "    back_to_school_drop = age_distribution_adjusted[0]\n",
    "    age_distribution_adjusted[0] = 0\n",
    "\n",
    "# Normalize the adjusted distribution to ensure it sums to 1\n",
    "age_distribution_adjusted = np.array(age_distribution_adjusted) / np.sum(age_distribution_adjusted)\n",
    "age_groups = np.random.choice(['Under 18', '18-34', '35-54', '55+'], size=n_users, p=age_distribution_adjusted)\n",
    "\n",
    "# Assign skill levels\n",
    "skill_levels = np.random.choice(['Beginner', 'Intermediate', 'Advanced'], size=n_users, p=skill_distribution)\n",
    "\n",
    "# Assign instruments\n",
    "instruments = np.random.choice(['Guitar', 'Piano', 'Ukulele', 'Bass', 'Voice'], size=n_users, p=instrument_distribution)\n",
    "\n",
    "# Assign countries\n",
    "countries = random.choices(country_list, weights=country_weights, k=n_users)\n",
    "print(f\"Attributes succesfully generated\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86643a3f-749f-41f0-958c-c7ffcc3bff3b",
   "metadata": {},
   "source": [
    "# Churn Rates\n",
    "Churn rates reflect the likelihood of users disengaging from the platform. These are influenced by:\n",
    "1. **Age Groups**: Younger users (Under 18, 18-34) churn more frequently.\n",
    "2. **Instruments**: Instruments with broader appeal (Guitar, Piano) have lower churn rates than niche ones (Voice).\n",
    "3. **Retention Behavior**: Users with higher Day 30 retention have a lower chance of churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b943614-5548-43c8-bfb1-e85bd44f3d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Churn rates succesfully created\n",
      "\n",
      "Churn: 4791\n",
      "Churn rate: 15.5%\n"
     ]
    }
   ],
   "source": [
    "SEED = 301 \n",
    "np.random.seed(SEED)\n",
    "churn_scalers_age = {'Under 18': 0.6, '18-34': 0.5, '35-54': 0.4, '55+': 0.3}\n",
    "instrument_engagement = {'Guitar': 0.3, 'Piano': 0.35, 'Ukulele': 0.4, 'Bass': 0.45, 'Voice': 0.5}\n",
    "\n",
    "# Define churn probabilities based on instruments\n",
    "instrument_engagement = {'Guitar': 0.2, 'Piano': 0.25, 'Ukulele': 0.3, 'Bass': 0.35, 'Voice': 0.4}\n",
    "churn_prob_instrument = [instrument_engagement[instrument] for instrument in instruments]\n",
    "noise_instrument = np.random.normal(0, 0.15, len(churn_prob_instrument))  # Noise with SD=0.15\n",
    "\n",
    "# Add random noise to churn probabilities\n",
    "churn_prob_age = [churn_scalers_age[age] for age in age_groups]\n",
    "noise_age = np.random.normal(0, 0.1, len(churn_prob_age))  # Noise with SD=0.1\n",
    "churn_prob_age_noisy = np.clip(np.array(churn_prob_age) + noise_age, 0, 1)\n",
    "\n",
    "noise_instrument = np.random.normal(0, 0.05, len(churn_prob_instrument))  # Noise with SD=0.05\n",
    "churn_prob_instrument_noisy = np.clip(np.array(churn_prob_instrument) + noise_instrument, 0, 1)\n",
    "\n",
    "# Combine churn probabilities\n",
    "churn_prob = churn_prob_age_noisy * churn_prob_instrument_noisy\n",
    "\n",
    "# Add country-specific effects\n",
    "min_weight = min(country_weight_dict.values())\n",
    "max_weight = max(country_weight_dict.values())\n",
    "country_weight_norm = {\n",
    "    country: 0.5 + (weight - min_weight) / (max_weight - min_weight) * 1.0  # Scale to [0.5, 1.5]\n",
    "    for country, weight in country_weight_dict.items()\n",
    "}\n",
    "\n",
    "\n",
    "# Compute interaction term with country weights factored in\n",
    "interaction_term = np.array([\n",
    "    (0.7 if age == '18-34' and instrument == 'Guitar' else \n",
    "     1.3 if age == 'Under 18' and instrument == 'Voice' else \n",
    "     1.1 if age == '35-54' and instrument in ['Piano', 'Ukulele'] else \n",
    "     1.0) * country_weight_norm.get(country, 1.0)  # Default to 1.0 if country is missing\n",
    "    for age, instrument, country in zip(age_groups, instruments, countries)\n",
    "])\n",
    "churn_prob *= interaction_term\n",
    "\n",
    "# Add churn spikes for a random 5% of users\n",
    "spike_indices = np.random.choice(len(churn_prob), size=int(len(churn_prob) * 0.1), replace=False)\n",
    "churn_prob[spike_indices] = np.clip(churn_prob[spike_indices] + np.random.uniform(0.4, 0.6, len(spike_indices)), 0, 1)\n",
    "\n",
    "# Simulate churn\n",
    "churn = np.random.binomial(1, churn_prob)\n",
    "print(f\"Churn rates succesfully created\\n\") \n",
    "# Create dataset\n",
    "dataset = pd.DataFrame({\n",
    "    'user_id': [f\"AUG{str(i).zfill(5)}\" for i in range(1, n_users + 1)],\n",
    "    'start_date':start_dates,\n",
    "    'age_group': age_groups,\n",
    "    'instrument': instruments,\n",
    "    'country': countries,\n",
    "    'skill_level': skill_levels,\n",
    "    'churn': churn\n",
    "})\n",
    "dataset.head(3)\n",
    "churn = dataset['churn'].sum() \n",
    "churn_rate = churn/dataset['churn'].size\n",
    "print(f\"Churn: {churn}\") \n",
    "print(f\"Churn rate: {round(churn_rate,3)*100}%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d754871-eaa4-46ea-ace3-d1a88f81013a",
   "metadata": {},
   "source": [
    "# Retention Metrics\n",
    "Retention metrics track user engagement at different time periods (Day 1, Day 7, Day 30). These are influenced by:\n",
    "1. **Age Group**: Older users (35-54, 55+) are typically more consistent.\n",
    "2. **Instrument**: Instruments like Guitar and Piano encourage more consistent retention.\n",
    "3. **Country**: Users from stronger economies tend to retain longer due to better resources.\n",
    "4. **Outliers**: Retention extremes, such as users retained across all periods, are introduced for realism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc5122c5-d5b9-4dd8-8cf3-bb42ba83b50a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Day 1 Probabilities (mean, min, max): 0.1965616946120591 0.05 0.8723096306705631\n",
      "Adjusted Day 7 Probabilities (mean, min, max): 0.20897759947329453 0.01638787878787879 0.52\n",
      "Adjusted Day 30 Probabilities (mean, min, max): 0.14604988963189702 0.011453131313131316 0.36341666666666667\n",
      "Retention: 9231\n",
      "Non-retention: 83313\n",
      "Retention rate: 9.97%\n"
     ]
    }
   ],
   "source": [
    "SEED = 301 \n",
    "np.random.seed(SEED)\n",
    "# Define retention scalers for factors\n",
    "retention_scalers_age = {'Under 18': 0.8, '18-34': 1.0, '35-54': 1.5, '55+': 1.2}\n",
    "retention_scalers_instrument = {'Guitar': 1.2, 'Piano': 1.1, 'Ukulele': 1.0, 'Bass': 0.9, 'Voice': 0.8}\n",
    "retention_scalers_country = {\n",
    "    country: round(scaler / 10 * round_float(1, 1.5), 2)\n",
    "    for country, scaler in zip(country_list, country_weights)\n",
    "}\n",
    "\n",
    "# Define base retention probabilities\n",
    "day_1_prob = round_float(0.3, 0.4)  # Adjusted upwards\n",
    "day_7_given_day_1_prob = round_float(0.5, 0.6)  # Adjusted upwards\n",
    "day_30_given_day_7_prob = round_float(0.4, 0.5)  # Adjusted upwards\n",
    "\n",
    "# Add noise to base retention probabilities\n",
    "day_1_prob_noisy = np.clip(day_1_prob * round_float(0.5, 1.5), 0, 1)\n",
    "day_7_given_day_1_prob_noisy = np.clip(day_7_given_day_1_prob * round_float(0.5, 1.5), 0, 1)\n",
    "day_30_given_day_7_prob_noisy = np.clip(day_30_given_day_7_prob * round_float(0.5, 1.5), 0, 1)\n",
    "\n",
    "# Interaction effects for retention probabilities\n",
    "interaction_effects_age = np.array([retention_scalers_age[age] for age in dataset['age_group']])\n",
    "interaction_effects_instrument = np.array([retention_scalers_instrument[instrument] for instrument in dataset['instrument']])\n",
    "interaction_effects_country = np.array([retention_scalers_country.get(country, 1.0) for country in dataset['country']])\n",
    "\n",
    "# Combine interaction effects and normalize tightly\n",
    "interaction_effects = interaction_effects_age * interaction_effects_instrument * interaction_effects_country\n",
    "interaction_effects /= (np.max(interaction_effects) * 1.2)  # Tighter normalization\n",
    "\n",
    "# Adjust retention probabilities with interaction effects\n",
    "adjusted_day_1_prob = np.clip(day_1_prob_noisy * interaction_effects, 0, 1)\n",
    "adjusted_day_7_given_day_1_prob = np.clip(day_7_given_day_1_prob_noisy * interaction_effects, 0, 1)\n",
    "adjusted_day_30_given_day_7_prob = np.clip(day_30_given_day_7_prob_noisy * interaction_effects, 0, 1)\n",
    "\n",
    "# Spike retention probabilities for 2% of users\n",
    "spike_indices = np.random.choice(len(dataset), size=int(len(dataset) * 0.02), replace=False)\n",
    "adjusted_day_1_prob[spike_indices] = np.clip(adjusted_day_1_prob[spike_indices] + np.random.uniform(0.2, 0.4, len(spike_indices)), 0, 1)\n",
    "\n",
    "# Drop retention probabilities for 1% of users\n",
    "drop_indices = np.random.choice(len(dataset), size=int(len(dataset) * 0.01), replace=False)\n",
    "adjusted_day_1_prob[drop_indices] -= np.random.uniform(0.05, 0.1, len(drop_indices))  # Reduced noise range\n",
    "\n",
    "# Ensure probabilities don't go too low\n",
    "adjusted_day_1_prob = np.clip(adjusted_day_1_prob, 0.05, 1)  # Set a minimum threshold\n",
    "\n",
    "# Simulate retention metrics\n",
    "dataset['day_1_retention'] = np.random.binomial(1, adjusted_day_1_prob)\n",
    "dataset['day_7_retention'] = np.where(\n",
    "    dataset['day_1_retention'] == 1,\n",
    "    np.random.binomial(1, adjusted_day_7_given_day_1_prob),\n",
    "    0\n",
    ")\n",
    "dataset['day_30_retention'] = np.where(\n",
    "    dataset['day_7_retention'] == 1,\n",
    "    np.random.binomial(1, adjusted_day_30_given_day_7_prob),\n",
    "    0\n",
    ")\n",
    "\n",
    "# Introduce retention outliers (users retained across all periods)\n",
    "n_users = len(dataset)\n",
    "retention_outliers = np.random.choice(dataset.index, size=int(n_users * 0.015), replace=False)  # Increased to 1.5%\n",
    "dataset.loc[retention_outliers, ['day_1_retention', 'day_7_retention', 'day_30_retention']] = 1\n",
    "\n",
    "# Retention metrics\n",
    "retention_rates = dataset[['day_1_retention', 'day_7_retention', 'day_30_retention']]\n",
    "retention = retention_rates.sum().sum()  # Total 1s\n",
    "non_retention = retention_rates.size - retention  # Total 0s\n",
    "total_entries = retention_rates.size  # Total number of values\n",
    "\n",
    "# Debugging intermediate values\n",
    "print(\"Adjusted Day 1 Probabilities (mean, min, max):\", adjusted_day_1_prob.mean(), adjusted_day_1_prob.min(), adjusted_day_1_prob.max())\n",
    "print(\"Adjusted Day 7 Probabilities (mean, min, max):\", adjusted_day_7_given_day_1_prob.mean(), adjusted_day_7_given_day_1_prob.min(), adjusted_day_7_given_day_1_prob.max())\n",
    "print(\"Adjusted Day 30 Probabilities (mean, min, max):\", adjusted_day_30_given_day_7_prob.mean(), adjusted_day_30_given_day_7_prob.min(), adjusted_day_30_given_day_7_prob.max())\n",
    "\n",
    "# Calculate retention rate\n",
    "retention_rate = round((retention / total_entries) * 100, 2)\n",
    "print(f\"Retention: {retention}\")\n",
    "print(f\"Non-retention: {non_retention}\")\n",
    "print(f\"Retention rate: {retention_rate}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d52804d-e389-428e-93cd-7f4a9b98afc1",
   "metadata": {},
   "source": [
    "# Fine-Tuning Premium Conversion\n",
    "Premium conversion is influenced by:\n",
    "1. **Age Groups**: Older users (35-54, 55+) convert more due to higher disposable income.\n",
    "2. **Country**: Stronger economies like the US and Germany have higher conversion rates.\n",
    "3. **Retention**: Users with higher Day 30 retention are more likely to convert.\n",
    "4. **Instrument Engagement**: Structured instruments like Piano tend to encourage premium conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "166505d7-f927-475b-8df0-a04782a2e4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1 Premium Conversion Rate (given Day 1 Retention): 1.64%\n",
      "Day 7 Premium Conversion Rate (given Day 7 Retention): 2.58%\n",
      "Day 30 Premium Conversion Rate (given Day 30 Retention): 2.21%\n",
      "Overall Premium Conversion Rate: 0.90%\n"
     ]
    }
   ],
   "source": [
    "SEED = 301 \n",
    "np.random.seed(SEED)\n",
    "# Premium conversion scalers\n",
    "premium_scalers_instrument = {'Guitar': 1.2, 'Piano': 1.1, 'Ukulele': 1.0, 'Bass': 0.9, 'Voice': 0.8}\n",
    "premium_scalers_age = {'Under 18': 0.8, '18-34': 1.0, '35-54': 1.2, '55+': 1.1}\n",
    "premium_scalers_country = {\n",
    "    country: round(scaler / 10 * round_float(0.8, 1.2), 2)\n",
    "    for country, scaler in zip(country_list, country_weights)\n",
    "}\n",
    "\n",
    "# Factor in instrument, age, and country scalers\n",
    "premium_effects_instrument = np.array([premium_scalers_instrument[instrument] for instrument in dataset['instrument']])\n",
    "premium_effects_age = np.array([premium_scalers_age[age] for age in dataset['age_group']])\n",
    "premium_effects_country = np.array([premium_scalers_country.get(country, 1.0) for country in dataset['country']])\n",
    "\n",
    "# Combine interaction effects\n",
    "premium_interaction_effects = premium_effects_instrument * premium_effects_age * premium_effects_country\n",
    "premium_interaction_effects /= np.max(premium_interaction_effects)  # Normalize\n",
    "\n",
    "# Adjust premium conversion probabilities with additional factors\n",
    "premium_conversion_prob = np.clip(\n",
    "    adjusted_day_1_prob * adjusted_day_7_given_day_1_prob * adjusted_day_30_given_day_7_prob * premium_interaction_effects,\n",
    "    0,\n",
    "    1\n",
    ")\n",
    "\n",
    "# Simulate premium conversion\n",
    "dataset['premium_conversion'] = np.random.binomial(1, premium_conversion_prob)\n",
    "\n",
    "# Calculate premium conversion metrics\n",
    "day_1_premium_conversion_rate = dataset.loc[dataset['day_1_retention'] == 1, 'premium_conversion'].mean()\n",
    "day_7_premium_conversion_rate = dataset.loc[dataset['day_7_retention'] == 1, 'premium_conversion'].mean()\n",
    "day_30_premium_conversion_rate = dataset.loc[dataset['day_30_retention'] == 1, 'premium_conversion'].mean()\n",
    "overall_premium_conversion_rate = dataset['premium_conversion'].mean()\n",
    "\n",
    "# Print premium conversion metrics\n",
    "print(f\"Day 1 Premium Conversion Rate (given Day 1 Retention): {day_1_premium_conversion_rate:.2%}\")\n",
    "print(f\"Day 7 Premium Conversion Rate (given Day 7 Retention): {day_7_premium_conversion_rate:.2%}\")\n",
    "print(f\"Day 30 Premium Conversion Rate (given Day 30 Retention): {day_30_premium_conversion_rate:.2%}\")\n",
    "print(f\"Overall Premium Conversion Rate: {overall_premium_conversion_rate:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1874f77d-540e-4bfe-b5e1-02765a037016",
   "metadata": {},
   "source": [
    "# Lessons Completed\n",
    "Lessons completed track user activity. The number of lessons depends on:\n",
    "1. **Retention**: Users retained longer complete more lessons.\n",
    "2. **Skill Level**: Advanced users may complete more lessons, but beginners might binge early.\n",
    "3. **Instrument**: Some instruments (e.g., Guitar, Piano) encourage higher engagement.\n",
    "4. **Outliers**: Power users completing an unusually high number of lessons are introduced for added realism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c27ebb9f-05d7-452c-b8fb-93ad6d34db69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lessons completed per user were successfully created\n",
      "Number of lessons completed: 368845\n",
      "Standard deviation: 10.342755572784496\n"
     ]
    }
   ],
   "source": [
    "SEED = 301 \n",
    "np.random.seed(SEED)\n",
    "# Define baseline lessons completed\n",
    "lessons_mean = {\n",
    "    'Beginner': random.randint(5, 10),  # Randomized mean for Beginner\n",
    "    'Intermediate': random.randint(10, 15),  # Randomized mean for Intermediate\n",
    "    'Advanced': random.randint(15, 20)  # Randomized mean for Advanced\n",
    "}\n",
    "lessons_std_dev = {\n",
    "    'Beginner': random.randint(5, 10),  # Randomized standard deviation for Beginner\n",
    "    'Intermediate': random.randint(10, 15),  # Randomized standard deviation for Intermediate\n",
    "    'Advanced': random.randint(15, 20)  # Randomized standard deviation for Advanced\n",
    "}\n",
    "\n",
    "# Simulate lessons completed\n",
    "lessons_completed = [\n",
    "    np.random.normal(\n",
    "        lessons_mean[skill_level] * (1 + 0.1 * retention),  # Scale by retention\n",
    "        lessons_std_dev[skill_level]  # Standard deviation per skill level\n",
    "    )\n",
    "    for skill_level, retention in zip(dataset['skill_level'], dataset['day_30_retention'])\n",
    "]\n",
    "dataset['lessons_completed'] = np.clip(np.round(lessons_completed), 0, None).astype(int)\n",
    "\n",
    "# Introduce lessons completed outliers\n",
    "lesson_outliers = np.random.choice(dataset.index, size=int(n_users * 0.01), replace=False)\n",
    "dataset.loc[lesson_outliers, 'lessons_completed'] += np.random.randint(10, 50, len(lesson_outliers))\n",
    "print(f\"Lessons completed per user were successfully created\") \n",
    "print(f\"Number of lessons completed: {dataset['lessons_completed'].sum()}\")\n",
    "print(f\"Standard deviation: {dataset['lessons_completed'].std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5a56e6-9d9a-4d82-9b43-b1709b941916",
   "metadata": {},
   "source": [
    "# Power Users\n",
    "Power users are individuals with unusually high engagement, completing significantly more lessons than average. These users add realism and test the dataset's ability to handle extreme cases.\n",
    "\n",
    "# Idle Users  \n",
    "Idle users are individuals who register on the platform but exhibit minimal or no engagement, completing zero or very few lessons. These users reflect a common real-world scenario where many people sign up out of curiosity but do not actively use the platform.  \n",
    "\n",
    "#### Justification  \n",
    "- **Realistic Representation:** Including idle users ensures the dataset mirrors actual platform usage patterns, where a significant percentage of users fail to engage meaningfully.  \n",
    "- **Business Insights:** Identifying and analyzing idle users helps businesses understand barriers to engagement, allowing them to design targeted strategies for reactivation.  \n",
    "- **Performance Testing:** Idle users test the platform's ability to handle inactive accounts without impacting metrics like conversion rates, retention, or scalability.  \n",
    "- **Strategic Planning:** Incorporating idle users highlights the need for outreach campaigns, personalized onboarding, or incentive programs to convert these users into active ones.  \n",
    "\n",
    "By including idle users in the dataset, the simulation gains credibility and creates opportunities for actionable insights into user engagement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69b1a446-02a9-479e-97dc-77e52c835c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idle and Power Users successfully created\n"
     ]
    }
   ],
   "source": [
    "SEED = 301 \n",
    "np.random.seed(SEED)\n",
    "lessons_mean = {'Beginner': 5, 'Intermediate': 7, 'Advanced': 10}\n",
    "lessons_std_dev = {'Beginner': 2, 'Intermediate': 3, 'Advanced': 4}\n",
    "\n",
    "lessons_completed = [\n",
    "    np.random.normal(\n",
    "        lessons_mean[skill_level], lessons_std_dev[skill_level]\n",
    "    )\n",
    "    for skill_level in dataset['skill_level']\n",
    "]\n",
    "dataset['lessons_completed'] = np.clip(np.round(lessons_completed), 0, None).astype(int)\n",
    "\n",
    "# Identify power users\n",
    "power_user_indices = np.random.choice(dataset.index, size=int(n_users * 0.01), replace=False)\n",
    "idle_users_indices = np.random.choice(dataset.index, size=int(n_users * 0.15), replace=False)\n",
    "\n",
    "# Add extreme lessons for power users\n",
    "dataset.loc[power_user_indices, 'lessons_completed'] += np.random.randint(10, 50, len(power_user_indices))\n",
    "dataset.loc[idle_users_indices, 'lessons_completed'] = np.random.randint(0, 4, len(idle_users_indices))\n",
    "print(f\"Idle and Power Users successfully created\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fa4fa9-9601-410a-91a9-cebcf2eeb277",
   "metadata": {},
   "source": [
    "# Geographic Anomalies\n",
    "Geographic anomalies represent rare or unexpected patterns, such as users from underrepresented countries with unusually high retention or conversion rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62bf478c-9110-4fb4-9bb9-3e950ab45768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographic anomalies successfully created'\n"
     ]
    }
   ],
   "source": [
    "SEED = 301 \n",
    "np.random.seed(SEED)\n",
    "# Expanded list of rare countries\n",
    "rare_countries = [\n",
    "    'Maldives', 'Bhutan', 'Timor-Leste',  # Asia\n",
    "    'Tuvalu', 'Nauru', 'Palau',          # Oceania\n",
    "    'San Marino', 'Liechtenstein', 'Monaco',  # Europe\n",
    "    'Seychelles', 'Djibouti', 'Eswatini',    # Africa\n",
    "    'Guyana', 'Suriname', 'Paraguay',    # South America\n",
    "    'Belize', 'Barbados', 'Saint Kitts and Nevis'  # North America\n",
    "]\n",
    "\n",
    "# Assign rare countries to 2-3% of users\n",
    "rare_indices = np.random.choice(dataset.index, size=int(len(dataset) * 0.03), replace=False)\n",
    "dataset.loc[rare_indices, 'country'] = np.random.choice(rare_countries, size=len(rare_indices))\n",
    "\n",
    "# Retention anomalies for rare countries\n",
    "high_retention_countries = ['Maldives', 'Bhutan', 'San Marino', 'Liechtenstein', 'Seychelles']\n",
    "low_retention_countries = ['Tuvalu', 'Nauru', 'Timor-Leste', 'Djibouti', 'Eswatini']\n",
    "\n",
    "# Boost or drop retention based on country\n",
    "dataset.loc[dataset['country'].isin(high_retention_countries), 'day_30_retention'] = np.random.binomial(\n",
    "    1, 0.9, len(dataset[dataset['country'].isin(high_retention_countries)])\n",
    ")\n",
    "dataset.loc[dataset['country'].isin(low_retention_countries), 'day_30_retention'] = np.random.binomial(\n",
    "    1, 0.3, len(dataset[dataset['country'].isin(low_retention_countries)])\n",
    ")\n",
    "\n",
    "# Premium conversion anomalies\n",
    "high_conversion_countries = ['Monaco', 'Palau', 'Barbados', 'San Marino']\n",
    "low_conversion_countries = ['Tuvalu', 'Djibouti', 'Timor-Leste', 'Eswatini']\n",
    "\n",
    "# Apply conversion rates based on country\n",
    "dataset.loc[dataset['country'].isin(high_conversion_countries), 'premium_conversion'] = np.random.binomial(\n",
    "    1, 0.8, len(dataset[dataset['country'].isin(high_conversion_countries)])\n",
    ")\n",
    "dataset.loc[dataset['country'].isin(low_conversion_countries), 'premium_conversion'] = np.random.binomial(\n",
    "    1, 0.2, len(dataset[dataset['country'].isin(low_conversion_countries)])\n",
    ")\n",
    "\n",
    "# Lessons completed anomalies\n",
    "high_activity_countries = ['Bhutan', 'Seychelles', 'Monaco', 'Liechtenstein']\n",
    "low_activity_countries = ['Tuvalu', 'Guyana', 'Paraguay', 'Eswatini']\n",
    "\n",
    "# Boost or reduce lessons completed\n",
    "dataset.loc[dataset['country'].isin(high_activity_countries), 'lessons_completed'] += np.random.randint(\n",
    "    10, 50, len(dataset[dataset['country'].isin(high_activity_countries)])\n",
    ")\n",
    "dataset.loc[dataset['country'].isin(low_activity_countries), 'lessons_completed'] -= np.random.randint(\n",
    "    1, 10, len(dataset[dataset['country'].isin(low_activity_countries)])\n",
    ")\n",
    "dataset['lessons_completed'] = np.clip(dataset['lessons_completed'], 0, None)  # Ensure no negative values\n",
    "print(\"Geographic anomalies successfully created'\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb3a0c3-1d69-45a4-8861-0c5f1c3a0410",
   "metadata": {},
   "source": [
    "# Instrument Extremes\n",
    "Certain users exhibit disproportionate activity for specific instruments. These outliers represent highly engaged users who skew the typical engagement patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f43e519f-ebdd-4ccd-9405-8d2286510d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instrument extremes successfully created'\n"
     ]
    }
   ],
   "source": [
    "SEED = 301 \n",
    "np.random.seed(SEED)\n",
    "# Instrument outlier counts with variability\n",
    "instrument_outlier_base = {\n",
    "    'Guitar': 0.01, 'Piano': 0.01, 'Ukulele': 0.005, 'Bass': 0.005, 'Voice': 0.003\n",
    "}\n",
    "\n",
    "# Add variability to outlier proportions\n",
    "instrument_outlier_counts = {\n",
    "    instrument: max(0, proportion + np.random.uniform(-0.002, 0.002))  # Add noise to proportions\n",
    "    for instrument, proportion in instrument_outlier_base.items()\n",
    "}\n",
    "\n",
    "# Introduce both high and low outliers\n",
    "for instrument, proportion in instrument_outlier_counts.items():\n",
    "    # Get indices for outliers\n",
    "    outlier_indices = dataset[dataset['instrument'] == instrument].sample(\n",
    "        frac=proportion, random_state=42\n",
    "    ).index\n",
    "    \n",
    "    # Randomly decide high or low outliers for each user\n",
    "    for idx in outlier_indices:\n",
    "        if np.random.rand() > 0.5:  # 50% chance of being a high outlier\n",
    "            dataset.loc[idx, 'lessons_completed'] += np.random.randint(15, 40)\n",
    "        else:  # 50% chance of being a low outlier\n",
    "            dataset.loc[idx, 'lessons_completed'] -= np.random.randint(5, 15)\n",
    "\n",
    "# Ensure lessons_completed remains non-negative\n",
    "dataset['lessons_completed'] = np.clip(dataset['lessons_completed'], 0, None)\n",
    "print(\"Instrument extremes successfully created'\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7b2612-42c5-4c66-82b5-80aac5feefa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2815f71b-006e-4b28-bdcd-ebb04e376c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>start_date</th>\n",
       "      <th>age_group</th>\n",
       "      <th>instrument</th>\n",
       "      <th>country</th>\n",
       "      <th>skill_level</th>\n",
       "      <th>churn</th>\n",
       "      <th>day_1_retention</th>\n",
       "      <th>day_7_retention</th>\n",
       "      <th>day_30_retention</th>\n",
       "      <th>premium_conversion</th>\n",
       "      <th>lessons_completed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AUG00001</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>18-34</td>\n",
       "      <td>Guitar</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AUG00002</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>35-54</td>\n",
       "      <td>Guitar</td>\n",
       "      <td>United States</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AUG00003</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>35-54</td>\n",
       "      <td>Guitar</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AUG00004</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>18-34</td>\n",
       "      <td>Guitar</td>\n",
       "      <td>Seychelles</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AUG00005</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>35-54</td>\n",
       "      <td>Guitar</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id start_date age_group instrument         country   skill_level  \\\n",
       "0  AUG00001 2024-08-01     18-34     Guitar          Mexico      Beginner   \n",
       "1  AUG00002 2024-08-01     35-54     Guitar   United States      Beginner   \n",
       "2  AUG00003 2024-08-01     35-54     Guitar  United Kingdom      Beginner   \n",
       "3  AUG00004 2024-08-01     18-34     Guitar      Seychelles      Beginner   \n",
       "4  AUG00005 2024-08-01     35-54     Guitar          Mexico  Intermediate   \n",
       "\n",
       "   churn  day_1_retention  day_7_retention  day_30_retention  \\\n",
       "0      0                0                0                 0   \n",
       "1      0                1                1                 0   \n",
       "2      0                0                0                 0   \n",
       "3      0                1                0                 1   \n",
       "4      0                1                0                 0   \n",
       "\n",
       "   premium_conversion  lessons_completed  \n",
       "0                   0                  7  \n",
       "1                   1                  2  \n",
       "2                   0                  6  \n",
       "3                   1                 53  \n",
       "4                   0                  3  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None) \n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da5a6768-dc81-4fce-8350-fbd480375fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('pretest.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aa8bdf-f241-41ef-b33d-afec266e4e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd0d28a-9da0-4e16-9c62-4a39af71e921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
